{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df_AAPL =  pd.read_csv('../data/AAPL.csv', header=0)\n",
    " # save it to a new CSV file\n",
    "\n",
    "df_AAPL.head()\n",
    "\n",
    "print(df_AAPL.shape)\n",
    "\n",
    "\n",
    "# exploring the data\n",
    "print(df_AAPL.describe())\n",
    "print(df_AAPL.isnull().sum())\n",
    "#setting the index of the dataframe to the date column\n",
    "df_AAPL['Date'] = pd.to_datetime(df_AAPL['Date'], format='%d-%m-%Y')\n",
    "\n",
    "\n",
    "print(df_AAPL.Date.dtype)\n",
    "\n",
    "df_AAPL = df_AAPL.set_index(\"Date\")\n",
    "\n",
    "df_AAPL.head(50)\n",
    "#setting the frequency of the index to business days\n",
    "print(df_AAPL.shape)\n",
    "df_AAPL.index.duplicated().sum()\n",
    "df_AAPL = df_AAPL[~df_AAPL.index.duplicated(keep='first')]\n",
    "AAPL_data = df_AAPL.asfreq('b')\n",
    "print(AAPL_data.shape)\n",
    "\n",
    "print(AAPL_data.tail(30))\n",
    "\n",
    "print(AAPL_data.isnull().sum())\n",
    "\n",
    "\n",
    "#getting the dates of the missing values\n",
    "null_data = AAPL_data[AAPL_data.isnull().any(axis=1)]\n",
    "\n",
    "null_data.head()\n",
    "\n",
    "null_dates = null_data.index.tolist()\n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "\n",
    "holidays = []\n",
    "\n",
    "# A complete list of Good Fridays for the relevant period\n",
    "good_fridays_list = [\n",
    "    datetime.date(2006, 4, 14), datetime.date(2007, 4, 6),\n",
    "    datetime.date(2008, 3, 21), datetime.date(2009, 4, 10),\n",
    "    datetime.date(2010, 4, 2),  datetime.date(2011, 4, 22),\n",
    "    datetime.date(2012, 4, 6),  datetime.date(2013, 3, 29),\n",
    "    datetime.date(2014, 4, 18), datetime.date(2015, 4, 3),\n",
    "    datetime.date(2016, 3, 25), datetime.date(2017, 4, 14),\n",
    "    datetime.date(2018, 3, 30), datetime.date(2019, 4, 19),\n",
    "    datetime.date(2020, 4, 10)\n",
    "]\n",
    "good_fridays = [pd.to_datetime(date) for date in good_fridays_list]\n",
    "\n",
    "# Special closures not covered by simple rules\n",
    "special_closures = [\n",
    "    pd.to_datetime('2007-01-02'), #mournday\n",
    "     pd.to_datetime('2012-10-29'), # Hurricane Sandy\n",
    "    pd.to_datetime('2012-10-30'), # Hurricane Sandy\n",
    "    pd.to_datetime('2018-12-05')  # National Day of Mourning\n",
    "]\n",
    "\n",
    "\n",
    "for date in null_dates:\n",
    "    year, month, day = date.year, date.month, date.day\n",
    "    week_day = calendar.day_name[date.weekday()]\n",
    "\n",
    "    # Check for special closures first\n",
    "    if date in special_closures:\n",
    "        holidays.append(date)\n",
    "        continue\n",
    "\n",
    "    # Check for Good Friday\n",
    "    if date in good_fridays:\n",
    "        holidays.append(date)\n",
    "        continue\n",
    "\n",
    "    # Check for standard holidays based on rules\n",
    "    if month == 1 and (day == 1 or (day == 2 and week_day == 'Monday')): # New Year's\n",
    "        holidays.append(date)\n",
    "    elif month == 1 and week_day == 'Monday' and 15 <= day <= 21: # MLK Day\n",
    "        holidays.append(date)\n",
    "    elif month == 2 and week_day == 'Monday' and 15 <= day <= 21: # Presidents' Day\n",
    "        holidays.append(date)\n",
    "    elif month == 5 and week_day == 'Monday' and day >= 25: # Memorial Day\n",
    "        holidays.append(date)\n",
    "    elif month == 7 and (day == 4 or (day == 5 and week_day == 'Monday') or (day == 3 and week_day == 'Friday')): # Independence Day\n",
    "        holidays.append(date)\n",
    "    elif month == 9 and week_day == 'Monday' and day <= 7: # Labor Day\n",
    "        holidays.append(date)\n",
    "    elif month == 11 and week_day == 'Thursday' and 22 <= day <= 28: # Thanksgiving\n",
    "        holidays.append(date)\n",
    "    elif month == 12 and (day == 25 or (day == 24 and week_day == 'Friday') or (day == 26 and week_day == 'Monday')): # Christmas\n",
    "        holidays.append(date)\n",
    "\n",
    "\n",
    "# Filter out the holidays to find any remaining non-holiday gaps \n",
    "non_holidays = [date for date in null_dates if date not in holidays]\n",
    "\n",
    "print(f\"Identified {len(holidays)} holidays within the missing dates.\")\n",
    "print(f\"Found {len(non_holidays)} missing dates that were NOT holidays.\")\n",
    "\n",
    "if non_holidays:\n",
    "    print(\"\\nThe following missing dates were NOT identified as holidays:\")\n",
    "    for date in non_holidays:\n",
    "        print(date.strftime('%Y-%m-%d'))\n",
    "else:\n",
    "    print(\"\\nSuccess! All missing dates were confirmed to be holidays or special closures.\")\n",
    "#getting the modified data\n",
    "print(AAPL_data.shape)\n",
    "\n",
    "modified_df = AAPL_data.drop(holidays)\n",
    "modified_df.shape\n",
    "print(\"Before filling missing values:\\n\",modified_df.isna().sum())\n",
    "\n",
    "modified_df = modified_df.bfill(axis ='rows')\n",
    "\n",
    "print(\"\\nAfter filling missing values:\\n\",modified_df.isna().sum())\n",
    "#visualizing the AAPLE Data\n",
    "def plotter(code):\n",
    "    global closing_stock\n",
    "    plt.subplot(211)\n",
    "    company_close = modified_df\n",
    "    company_close = company_close.Close.values.astype('float32')\n",
    "    company_close = company_close.reshape(-1, 1)\n",
    "    closing_stock = company_close\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(code + \" close stock prices\")\n",
    "    plt.title('prices Vs Time')\n",
    "    plt.grid(True)\n",
    "    plt.plot(company_close , 'b')\n",
    "    plt.show()\n",
    "\n",
    "plotter(\"AAPL\")\n",
    "#split the data\n",
    "n_train = int(len(closing_stock) * 0.80)\n",
    "n_remaining = len(closing_stock) - n_train\n",
    "\n",
    "n_val = int(n_remaining*0.50)\n",
    "n_test = n_remaining - n_val \n",
    "print(\"Train samples:\",n_train, \"Validation Samples:\",n_val,\"Test Samples:\", n_test)\n",
    "\n",
    "train_data = closing_stock[0:n_train]\n",
    "print(train_data.shape)\n",
    "\n",
    "val_data = closing_stock[n_train:n_train + n_val]\n",
    "print(val_data.shape)\n",
    "\n",
    "test_data = closing_stock[n_train + n_val:]\n",
    "print(test_data.shape)\n",
    "#feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train = scaler.fit_transform(train_data)\n",
    "val = scaler.transform(val_data)\n",
    "test = scaler.transform(test_data)\n",
    "#creating the dataset\n",
    "def create_dataset(data , n_features):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(data)-n_features-1):\n",
    "        a = data[i:(i+n_features), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(data[i + n_features, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "n_features = 2\n",
    "\n",
    "trainX, trainY = create_dataset(train, n_features)\n",
    "valX, valY = create_dataset(val, n_features)\n",
    "testX, testY = create_dataset(test, n_features)\n",
    "\n",
    "print(trainX.shape , trainY.shape , valX.shape , valY.shape, testX.shape , testY.shape)\n",
    "\n",
    "trainX = trainX.reshape(trainX.shape[0] , 1 ,trainX.shape[1])\n",
    "valX = valX.reshape(valX.shape[0] , 1 ,valX.shape[1])\n",
    "testX = testX.reshape(testX.shape[0] , 1 ,testX.shape[1])\n",
    "\n",
    "print(trainX.shape , trainY.shape , valX.shape , valY.shape, testX.shape , testY.shape)\n",
    "#building the model\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# First GRU layer\n",
    "model.add(layers.GRU(units=100, return_sequences=True, input_shape=(1,n_features), activation='tanh'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Second GRU layer\n",
    "model.add(layers.GRU(units=150, return_sequences=True,  activation='tanh'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Third GRU layer\n",
    "model.add(layers.GRU(units=100, activation='tanh'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# The output layer\n",
    "model.add(layers.Dense(units=1, kernel_initializer='he_uniform', activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate = 0.0005) , metrics = ['mean_squared_error'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(trainX,trainY,epochs=100,batch_size=128, verbose=1, validation_data = (valX,valY))\n",
    "#evaluating the model performance\n",
    "import math\n",
    "\n",
    "def model_score(model, X_train, y_train, X_val, y_val , X_test, y_test):\n",
    "    print('Train Score:')\n",
    "    train_score = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print(\"MSE: {:.5f} , RMSE: {:.2f}\".format(train_score[0], math.sqrt(train_score[0])))\n",
    "\n",
    "    print('Validation Score:')\n",
    "    val_score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(\"MSE: {:.5f} , RMSE: {:.2f}\".format (val_score[0], math.sqrt(val_score[0])))\n",
    "\n",
    "    print('Test Score:')\n",
    "    test_score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE: {:.5f} , RMSE: {:.2f}\".format (test_score[0], math.sqrt(test_score[0])))\n",
    "\n",
    "\n",
    "model_score(model, trainX, trainY ,valX, valY , testX, testY)\n",
    "#visualizing the loss vs epochs\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "#visualizing the predictions vs ground truth\n",
    "pred = model.predict(testX)\n",
    "pred = scaler.inverse_transform(pred)\n",
    "print(pred[:10])\n",
    "\n",
    "testY_actual = testY.reshape(testY.shape[0] , 1)\n",
    "testY_actual = scaler.inverse_transform(testY_actual)\n",
    "print(testY_actual[:10])\n",
    "\n",
    "plt.plot(testY_actual , 'b')\n",
    "plt.plot(pred , 'r')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Prices')\n",
    "plt.title('Check the performance of the model with time')\n",
    "plt.legend(['Actual', 'Predicted'], loc='upper left')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
